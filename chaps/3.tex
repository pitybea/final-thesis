\chapter{Efficient Detection by Combining of Appearance and Temporal Information }
\label{chp3}

\section{Introduction}

From time to time, efficient detection methods under limited computational power are very applicable in various areas, such as driving assistance using embedded sensors. Here, a method pursuing efficiency and real-time detection is proposed.
The proposed detection method makes use of both
appearance and motion information of the target objects. By well optimizing of the detecting pipeline, the
method works in real time, and gives 100\% detection rate and 0\% false alarm rate in one experiment of data collected by infrared cameras.

As always, detection performance and efficiency are the two important aspects of this method. Challenges come from the need to distinguish noisy objects as shown in Figure~\ref{fig:first} and the real-time requirement. In experiments, besides the target object a lot of noisy objects also exist. Some of the noisy objects cannot even by distinguished from the target objects by only appearance.  The clutter property of the sensed data makes the detection challenging. The proposed method meets this challenge by making use of both appearance and temporal information of the target objects. There are two main steps in the method.

The first step deals with keypoints, and  it takes original data as input, and outputs keypoint clusters as detection hypotheses. In this step, keypoints are detected, verified and then clustered. To detect keypoints, all points on each frame are uniformly sampled and filtered with pre-set intensity thresholds.  Then the keypoints are verified by a simple keypoint appearance model   powered by \emph{k}-means. At the end of the first step, the keypoints are clustered based on the Euclidean distance.

The second step takes the keypoint clusters as input, verifies them by appearance and temporal information, and outputs the ones pass verifications as detection results. In the second step, the keypoint clusters are labeled based on appearance by an adaboost machine, which is trained using intensity histograms of keypoint clusters from target objects and keypoint clusters from noisy objects. The keypoint clusters are also tracked by temporal association through frames. Motion information encoded in the trajectories are used to further verify the keypoint clusters. Finally, the keypoint clusters which pass both appearance and temporal verifications are decided as target objects.

This pipeline is designed also with consideration of the requirement for efficiency.  The method deals with the large amount information contained on one frame following a hierarchical manner. The later one step is, the more time-consuming it is  and the fewer instances it deals with. From an image containing $10^5$ pixels, $10^4$ points go through keypoint detection step of testing by intensity thresholds. Then in average, $10^3$ keypoints are detected, and verified, leaving about $10^2$ to be clustered. Afterwards, fewer than $10$ keypoint clusters are left, which are dealt with by the very time-consuming steps of generating image features and tracking.

The advantage of the method is its ability to give promising detection results from cluttered data in real time. Besides, the method is also a successful attempt to combine bottom-up and classification methods, and a successful attempt to combine both appearance and temporal information.


Firstly, the method is proposed with application scenario of data collected by infrared cameras, then it is also extended to corporate with data collected by ordinary cameras.
This chapter is organized as follows: section \ref{rw} reviews related work, section \ref{ab} introduces application background, section \ref{pip} proposes pipeline of the method, section \ref{exp} gives experimental results, section \ref{ord} gives results by extending the method for data collected by ordinary cameras, and section \ref{conc} concludes.














\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{17Orgimg00039.jpg}
\label{fig:first:a}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{17veriTrjimg00039.jpg}
\label{fig:first:b}}
\caption[Target objects and detection results]{Original data and detection results. In (a), the red arrow points to the target object: emergency telephone indicator, and the green arrows point to noisy objects. In (b), red rectangles mark detection hypotheses labeled as positive using appearance information, and green rectangles mark negative ones. Yellow trajectories mark detection hypotheses labeled as positive using temporal information, and white trajectories mark negative ones.}
\label{fig:first}
\end{figure}









\section{Related Work}
\label{rw}

Most modern detection methods fall into two categories. Some~\cite{ij4,ac31,ac30,ac4,ac32,ac29,ac28,ac1} follow the sliding-window schema, and they detect objects by consider whether each of the sub-images contains an instance of the target object. Classifiers are usually employed by these methods. The other methods~\cite{ac9,ac2,ac3,ac22,lb1,ac5,ac10,ac21,ac18} infer object centers based on local image features in a bottom-up manner. The proposed method makes advantages of both frameworks. Following the bottom-up manner, keypoints are detected, verified, and clustered. After these steps, the keypoint clusters are considered as detection hypotheses. Then following the sliding-window schema, the keypoint clusters are verified by their appearance and temporal information using discriminative methods.

Previous methods~\cite{ac34} also consider the combination of the two frameworks. Detection hypotheses are gained using Hough transform and then verified by support vector machines in~\cite{ac10,ac25}. The methods in~\cite{ac6,ac7}, use randomized decision trees for both decisions whether local features belonging to foreground objects or not and decisions of their Hough votes. The method proposed in~\cite{ac27} describes both frameworks in the same manner. While giving state-of-the-art detection performance, they can't meet the requirement for efficiency as this method does.

This work is also related to data association methods at time dimension. In tracking, the main attention is focused on solving a data
association problem to explain conflicts in data as well as
recovering from tracking failures within a low time cost. In
\cite{ij9}, the joint likelihood maximization is represented by the
Nash Equilibrium in a game. The main contribution is the time
complexity of solving a game-theoretic problem is low and make
trackers compete for the region that is in a conflict. In
\cite{ij10}, the MAP corresponds to the max flow of a well designed
graph. The method has the advantage of taking into consideration of
missed detections on middle frames and the low time cost. In
\cite{ij11}, the main idea is to firstly connect very faithful
detection response pairs, then solve the data association problem
via low-time-complexity Hungarian algorithm, and refine the result
in a EM manner at the last step. And the main concern of these work
is solving the high time complexity in the data association problem.

 This work is also related to feature grouping methods~\cite{ac25}, methods detecting using trajectories~\cite{my9,ac24}, tracking methods~\cite{my7,my10}, and methods integrating appearance and temporal information~\cite{ac23}.


\section{Application background}
\label{ab}

When developing the method, potential application scenarios are within consideration. The method aims to perform detection in real time, and serve as an effective unit in positioning automobiles in tunnel environment.

Positioning of automobiles acts an important role in autonomous driving, and it is also of great importance for driving assistance, vehicle navigation, etc. When GPS sensors function properly, it is usually easy for an automobile to estimate the position of  itself. While in tunnel environment, for most of the time, there is no GPS signals available. A new positioning system which functions properly in tunnel environment is very necessary~\cite{nig}.

In most tunnels which appear on express ways in Japan, there are lots of signs which appear at equal intervals. Attention can be focused on the emergency telephone indicators, which appear every 200 meters in tunnels. The absolute coordinates of the emergency telephone indicators can be obtained by the method of~\cite{xue}. While travelling in tunnels, if the emergency telephone indicators can be sensed, and the distance from the automobile to the indicators can be estimated, then the absolute position of the automobile can be inferred. Detection methods i.e.~\cite{ac23} based on ordinary cameras fail due to darkness. In experiments infrared cameras, which are usually equipped by recent intelligent vehicles and  suitable in dark environment, are used.

 In tunnel environment, besides the target objects, a lot of noisy objects also appear, e.g. ordinary lights, other vehicles, and other vehicles' shadows. So to well distinguish target objects is of importance. And such kind of applications usually require real-time detections. And the proposed method successfully meet the requirements.

\begin{comment}
Especially, compared with the method proposed in~\cite{wang1}, this method employs a more effective classifying machine by setting biased weights for positive and negative training examples, and far over-perform the method.
Inspired by a previous work~\cite{wang1}, the proposed method can be turned to a  approach to detecting emergency telephone indicators by using infrared cameras.
\end{comment}
\section{Detection Pipeline}
\label{pip}
The method can be considered as a two-step method. The first step deals with keypoints. It
takes original data as input, and outputs keypoint clusters as detection hypotheses. The second
step takes these keypoint clusters as input, verifies them by their appearance and motion
information, and outputs the ones which pass verifications as detection results.

\subsection{Keypoint Detection}
In data collected using ordinary cameras, keypoints~\cite{o2,o12} invariant to rotations, affine changes, and illumination changes are preferable. In the case of using data collected by infrared cameras, keypoint detection intends to provide hypotheses for target objects, i.e. emergency telephone indicators. Thus intensity is of great importance. This method employs a simple yet useful method to detect keypoints. Firstly, points are uniformly sampled with width step $W$, and height step $H$. In this manner the magnitude of instances is reduced by  at lease one order. Then the points are considered as keypoints if they pass the test which verifies them by setting intensity thresholds .

Here Gaussian distribution is assumed for the intensities of the points.


let$\{\bf{x}\}$ denote all the sampled points, $I_{\bf{x}}$ the intensity of each point, \begin{comment}$I_{\bf{x}}\sim{\mathcal{N}(\mu_{I_{\bf{x}}},{\sigma_{I_{\bf{x}}}}^2)}$,
\end{comment}
and $l_{\bf{x}}$ the label. If the point is considered as belonging to target objects, $l_{\bf{x}}=1$, otherwise, $l_{\bf{x}}=0$. By setting lower threshold, $I^{th1}_{\bf{x}}$,  and higher threshold, $I^{th2}_{\bf{x}}$, the probability that points belongs to target objects based on their falling into this interval is given by,
\begin{equation}
P(l_{\bf{x}}=1|I^{th1}_{\bf{x}}{\leq}I_x{\leq}{I^{th2}_{\bf{x}}})=
\frac
{P(l_{\bf{x}}=1,I^{th1}_{\bf{x}}{\leq}I_x{\leq}{I^{th2}_{\bf{x}}})} {P(I^{th1}_{\bf{x}}{\leq}I_x{\leq}{I^{th2}_{\bf{x}}})}\;.
\label{eq1}
\end{equation}



At this step,  that as few points belonging to the target objects as possible are excluded is also considered. The probability of ont point falling into the defined interval based on its belonging to target objects is given by,
\begin{equation}
P(I^{th1}_{\bf{x}}{\leq}I_x{\leq}{I^{th2}_{\bf{x}}}|l_{\bf{x}}=1)=
\frac
{P(l_{\bf{x}}=1,I^{th1}_{\bf{x}}{\leq}I_x{\leq}{I^{th2}_{\bf{x}}})} {P(l_{\bf{x}}=1)}\;.
\label{eq1.1}
\end{equation}
%del2
And points of which the intensities fall in the pre-set thresholds are detected as keypoints.
\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{17Kptimg00039.jpg}
\label{fig:sec:a}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{8Kptimg00028.jpg}
\label{fig:sec:b}}
\caption[Keypoint detection]{Keypoint detection. }
\label{fig:sec}
\end{figure}


\subsection{Keypoint Verification}
As shown in Figure \ref{fig:sec:b}, the detected keypoints don't just belong to target objects, but also belong to background. And for training, keypoints belonging to target objects are considered as positive ones, otherwise negative.

To verify the keypoints, the appearance of the sub-image around each keypoint is used. Intensity histograms are used to describe the appearance.
Because the noisy keypoints come from several sources, i.e., way of the tunnel, ordinary lights, and other vehicles in the case of emergency telephone indicator detection. Thus robust linear classifiers are not suitable for the verification. Here, a general model in the form of a simple mixture is used. The \emph{k}-means method is used to cluster the intensity histograms, $\{A_{\bf{x}},l_{\bf{x}}=1\}$, of the positive keypoints, and, $\{A_{\bf{x}},l_{\bf{x}}=0\}$,  of
the negative keypoints.


%del1

Let $\{C_1^i,i=1,2,...,n_1\}$ denote the intensity histogram centers of the positive keypoints, and $\{C_0^i,i=1,2,...,n_2\}$ the negative. For each $C_1$, the average Euclidean distance between $\{C_0^i,i=1,2,...,n_2\}$ is calculated as,
\begin{equation}
Eu(C_1^i)={\frac 1 n_2}\sum\limits^{n_2}_{j=1}Euclid(C_1^i,C_0^j)\;.
\label{eq2}
\end{equation}
Here, $Euclid(\cdot)$ calculates the Euclidean distance, and $Eu(\cdot)$ is a evaluation function of the positive feature centers. The positive feature centers are ranked by $Eu(\cdot)$, and the positive feature centers with largest $Eu(\cdot)$ are chosen and used for verification.


%del3
For verification, the intensity histogram of each keypoint's surrounding sub-image is extracted. Then the Euclidean distance between the extracted intensity histogram and its nearest positive feature center is calculated. If this distance exceeds a threshold, $D^{th}_{A_{\bf{x}}}$, it is considered as negative, else it is considered as positive. Here, for simplicity, unlike~\cite{ac33}, the same threshold is used for all components of the mixture.

\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{17VeriKptimg00039.jpg}
\label{fig:thir:a}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{8VeriKptimg00028.jpg}
\label{fig:thir:b}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{17Rgsimgwl00039.jpg}
\label{fig:thir:c}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{8Rgsimgwl00028.jpg}
\label{fig:thir:d}}
\caption[Keypoint verification and clustering]{Keypoint verification and clustering. Red circles mark keypoints which pass the verification, while blue marks failed ones. Rectangles mark keypoint clustering results. }
\label{fig:thir}
\end{figure}

\subsection{Keypoint Clustering}
After the keypoint verification step, on some frames, the result is pretty good, while on other frames, appearance of the keypoints is not enough for decisions of whether the keypoints belonging to target objects or not. Here generation of keypoint trajectories is not feasible, since nearby keypoints are similar in appearance and  the time complexity of associating such a large number of keypoints along time dimension is high. So the keypoints are clustered, and then data association in time dimension only need to deal with a small number of keypoint clusters.

To cluster the keypoints, a minimum spanning tree (mst) is built using the pairwise Euclidean distance between two keypoints. And the mst is split by cutting edges larger a threshold. This results in a grouping results of the keypoints, denoted by, $\gamma=\{\bf{g}\}$.


\subsection{Keypoint Cluster Verification by Appearance}

For each keypoint cluster, the smallest bounding rectangle is considered as detection hypothesis, as shown in Figure \ref{fig:thir:c} and Figure \ref{fig:thir:d}.

In the case of emergency telephone indicator detection, there are two main sources of noise. Ordinary lights are the first, and other vehicles with their shadows are the second. The global appearance of ordinary lights is different from that of the emergency telephone indicators. As ordinary lights get further from the infrared camera, the intensity of its corresponding sub-image in the collected data gets lower. At a certain distance, the intensity of the ordinary  lights is almost the same with emergency telephone indicators'. And for ordinary lights of which the intensity is higher than the emergency telephone indicators', the transition regions from them to tunnel walls will have similar intensity with the emergency telephone indicators.
This means though locally the emergency telephone indicators share the same appearance with ordinary lights, they can still be distinguished globally by appearance. As for other vehicles and their shadows, the intensity range of them is very close to the emergency telephone indicators', and they can hardly be distinguished just by appearance.



At this step, the keypoint clusters are verified by their appearance, aiming at filtering out keypoint clusters which do not share the same appearance model with the target objects. An Adaboost machine is trained using intensity histograms of the target objects and noisy objects. In the case of emergency telephone indicator detection, the appearance of other vehicles is close to the emergency telephone indicators', and they are not used for training the machine. For training of the machine, labeled 32-dimensional intensity histograms are firstly normalized. Then each weak classifier of the machine makes decision on one dimension of the intensity histograms. After this step, each keypoint cluster is either labeled as positive or negative.

In this step, to emphasize the Adaboost machine's performance on the positive training examples, the initial weights of the positive training examples are set to be much larger than the weights of the negative training examples.  Since in practice, whether each keypoint cluster is a target object or not is decided by both appearance and motion information. And the difficulties of exclude noisy objects can be left to the later steps.

\begin{figure}[b]
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{17Rgsimg00039.jpg}
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{8Rgsimg00028.jpg}
\caption[Keypoint cluster verification by appearance]{Keypoint cluster verification by appearance. Red rectangles: positive detection hypotheses, and green: negative detection hypotheses.}
\label{fig:fif}
\end{figure}

%test2
\subsection{Keypoint Cluster Tracking}
Not all noisy detection hypotheses can be excluded by using appearance, as shown in Figure \ref{fig:fif}. To distinguish keypoint clusters belonging to other vehicles and their shadows, the keypoint clusters are tracked through frames to generate trajectories.

In this case, the problem of keypoint cluster tracking  is relatively simple, since no occlusion occurs. To keep the method on-line and maintain efficiency, a pool of trajectories are kept, $\tau=\{T_{\bf{g}}^i,i=1,2,...,n\}$, and new detection hypotheses act as detection responses, $\nu=\{n_{\bf{g}}^i,i=1,2,...m\}$, in tracking. The problem of tracking is modeled as finding best data association hypothesis, $H^*$, between the trajectory set and detection response set as,
\begin{equation}
\begin{aligned}
{H^*} &= \mathop {\arg \max }\limits_{H \in \eta
} (P(H|\tau,\nu )) \\
&= \mathop {\arg \max }\limits_{H \in \eta }
(\prod\limits_{(T_{\bf{g}}^i,n_{\bf{g}}^j) \in H} {P_{link}(n_{\bf{g}}^j|T_{\bf{g}}^i)} ) \;.
\end{aligned}
\label{eq3}
\end{equation}

Let $u_{ij}=1 \mbox{ or } 0$ indicates $n_{\bf{g}}^j$ is linked to $T_{\bf{g}}^i$ or not, and assuming each trajectory can link once and each detection response can only be linked once, the problem can be modeled as,
\[
\begin{aligned}
&\arg \max\limits_{u_{ij}} \sum\limits_{i = 1}^n \sum\limits_{j = 1}^m u_{ij} \ln P_{link}(n_{\bf{g}}^j|T_{\bf{g}}^i)\\
&
\begin{aligned}
    s.t.:\mbox{ }&u_{ij}=0\mbox{ or }u_{ij}=1,\forall\;i,\forall\;j;\\
    &\sum\limits_{i = 1}^n {u_{ij}} \leq 1\;; \sum\limits_{j = 1}^m {u_{ij}}\leq 1\;.
\end{aligned}
\end{aligned}
\]
Here, $P_{link}(n_{\bf{g}}^j|T_{\bf{g}}^i)$ is defined by the appearance difference, the scale difference, and the time gap between the last detection response contained in $T_{\bf{g}}^i$ and $n_{\bf{g}}^j$. While Hungarian algorithm~\cite{ha} gives near-optimal solution, we follow a very simple manner for the solution by finding the best matched pairs and excluding them until no matching pairs can be found.

\subsection{Keypoint Cluster Verification by Motion}
As shown in Figure \ref{fig:sixs}, the trajectories from keypoint clusters belonging to target objects are different from other objects'. In this step, the temporal information encoded in the trajectories are used to further verify the keypoint clusters. In the case of emergence telephone indicator detection, a linear model is used to fit each trajectory, and the significance of the fitting is the criteria for decisions.

Let $(x^i_{\bf{g}},y^i_{\bf{g}})$ denote the coordinate of the $i$th element belonging to a trajectory. The linear assumption is that $y^i_{\bf{g}}=a_0+a_1{x^i_{\bf{g}}}$. The significance of the fitting is defined as,
\begin{equation}
r =| \frac{{\sum\limits_i {\left( {{x^i_{\bf{g}}} -  {\bar x_{\bf{g}}}} \right)\left( {{y^i_{\bf{g}}} -  {\bar y_{\bf{g}}}} \right)} }}{{{{\left[ {\sum\limits_i {{{\left( {{x^i_{\bf{g}}} -  {\bar x_{\bf{g}}}} \right)}^2} \cdot \sum\limits_i {{{\left( {{y^i_{\bf{g}}} -  {\bar y_{\bf{g}}}} \right)}^2}} } } \right]}^{1/2}}}}|\;.
\label{eq4}
\end{equation}
And the value of $r$ is used to decide the trajectories of the keypoint clusters as belonging to emergency telephone indicators or not.

\subsection{Pipeline Summary}
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
     \hline
     \hline
                               & \begin{tabular}{@{}c@{}}Appearance \\ /Motion\end{tabular} & \begin{tabular}{@{}c@{}}Online \\ /Offline\end{tabular} & \begin{tabular}{@{}c@{}}Discriminative \\ /Generative\end{tabular}  \\
    \hline
    Keypoint Detection         &	Appearance & Offline &  Generative  \\
    Keypoint Verification      & Appearance	 & Offline	& Generative \\
    Keypoint Clustering       &	 Appearance  & Online &  \\
    KC Verification by Appearance         &  Appearance    & Offline &  Discriminative	  \\
    KC Tracking                         & Motion & Online &           \\
    KC Verification by Motion           &  Motion    & Online &  Discriminative	   \\
   \hline
\end{tabular}
\caption[Pipeline summary]{Summary of all steps in the pipeline. Here, KC is short for keypoint cluster.}\label{c3tb:tb1}
\end{table}
Before proposing the decision step for detection, Table \ref{c3tb:tb1} summaries the steps in the pipeline from the type and style of the information source, and whether each step belongs to discriminative or generative methods. Generally speaking, motion information and discriminative methods are at later steps. This is because the computational cost is high, while discriminative methods need more information to guarantee performance. Online information are also mainly used in later steps, this is because without information provided by model trained offline, there is no instances to generate online information. Also the computational cost of online information is higher on the run.


\subsection{Object Detection}
For each keypoint cluster on the current frame, there exists label given by the Adaboost machine according to its appearance, and the significance of fitting its trajectory as a straight line.  For each keypoint cluster, it is considered as an target object if and only if its label given by the Adaboost machine is positive, its trajectory is longer than $l^{th}$, and the significance of fitting its trajectory into a straight line is larger than $r^{th}$.

Each trajectory not only connects the detection responses, but also connects the decisions for each detection responses made by their appearance and motion patterns. The target objects and noisy objects actually appear in successive frames, and even if we make a wrong decision on one frame, we can expect to recover from this mistake based on the results on other frames. The final results is based on the trajectories of decisions. When one trajectory ends, if most of the decisions it connects are positive, then this trajectory is considered as positive.



\section{Experimental Results}
\label{exp}
In this section, this method is tested on detection performance and efficiency. There are two experiments carried on, and results from both are reported.

\subsection{First Experiment}

\subsubsection{Data} For the first experiment, our experimental vehicle on top of which installed infrared cameras take several tours of the Awagatake tunnel. About 4,000 frames are collected for each tour. The frame size is $640\times 480$, the intensity range is [0,255], and the frame rate is 30 frames per second of the camera, and 15 frames per second of the collection program.

\subsubsection{Implementation Settings} All models are trained using data from one tour, while evaluated on data from another tour. Firstly, all emergency telephone indicators are marked in the form of rectangles on all frames from the training tour.

\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{siftimg00469.jpg}
\label{fig:sec:a}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{Kptimg00469.jpg}
\label{fig:sec:b}}
\caption[Keypoint detection comparison]{Keypoint detection. In (a), keypoint detection method in~\cite{o12} is used, and in (b), the keypoints are detected using the proposed method. And the proposed keypoint detection is more suitable for detecting keypoints belonging to emergency telephone indicators.}
\label{ex1:va}
\end{figure}

To set intensity thresholds for keypoint detection, Gaussian distribution is assumed for the points belonging to target objects. Width step $W$ set to 3, and $H$ set to 4. Keypoints in the marked rectangle are sampled from the frames of the training tour, and used to estimate $I^{th1}_{\bf{x}}$  and $I^{th2}_{\bf{x}}$.  Following the $3\sigma$ principle, $I^{th1}_{\bf{x}}$ is set to 160 and $I^{th2}_{\bf{x}}$, 190. In Figure \ref{ex1:va}, we also compare the keypoint detection results with the results using SIFT.

The keypoints used to set intensity thresholds, together with some keypoints randomly sampled from training data, which are out of the marked rectangles are used to train the mixture model, which is then used in the verification of the keypoints.
 Note this model and the training is not very accurate, since more accurate marking means more manual efforts. Also, future steps can filter out the false alarms produced by this step.  About 30,000 intensity histograms of the positive keypoints are sampled, and about 3,000,000 of the negative. When using of $k$-means for clustering the positive intensity histograms, $k$ is set to 40, and $k$ set to 400 for negative. The $k$ values over-segment both feature sets. The threshold to verify keypoints, $D^{th}_{A_{\bf{x}}}$ is set to 0.14 for the normalized histograms.


For keypoint clustering, the threshold to split the mst is set to 10, which is half the largest height of the emergency lights sensed by the camera.

\begin{figure}
\includegraphics[width=0.94\textwidth,bb=0 0 1367 651]{untitled.jpg}
\caption[Positive and negative training examples.]{Dimension reduction of intensity histograms manually marked as positive and negative, which have 2 dimensions by principle component analysis. Blue circles: positive, and red: negative.}
\label{ex1:v}
\end{figure}

 The Adaboost machine to distinguish other vehicles and their shadows is trained by intensity histograms of positive keypoint clusters and negative keypoint clusters. We manually mark 466 positive  and 1,421 negative keypoint clusters. And in Figure \ref{ex1:v}, show the image features of positive and negative training examples for the Adaboost machine, which are reduced to two dimensional by PCA. The trained Adaboost machine is tested on the training dataset, and the classification rate is 85\%.

During keypoint cluster tracking, whether a detection response can be linked to a trajectory or not is constrained by position and scale changes. Here scale change limit is set to 4. When the trajectories are fitted as lines, the linear model is also used in associating new detection responses.


In this experiment, $l^{th}$ is set to 5, $r^{th}$ is set to 0.7, and $R^{th}$ is set to 70\%. As the figures in the section for pipeline are all from the second experiment, here the results of each step of the data from the first experiment are also given. In Figure \ref{ex1:first}, the original data and sample detection results are given. In Figure \ref{ex1:thir}, the results from keypoint verification and clustering are given. In Figure \ref{ex1:four}, the results from keypoint cluster verification by appearance information are given. In Figure \ref{ex1:five}, the results from keypoint cluster tracking are given. And at last, in Figure \ref{ex1:sixs}, the final detection results are given.

\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{Orgimg00634.jpg}
\label{ex1:first:a}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{veriRegTrjimg00634.jpg}
\label{ex1:first:b}}
\caption[Original data and detection results]{Original data and detection results. Red arrow points to the target object in \ref{ex1:first:a}.}
\label{ex1:first}
\end{figure}

\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{VeriKptimg00469.jpg}
\label{fig:thir:a}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{VeriKptimg01080.jpg}
\label{fig:thir:b}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{Rgsimg00469.jpg}
\label{fig:thir:c}}
\subfigure[]{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{Rgsimg01080.jpg}
\label{fig:thir:d}}
\caption[Keypoint verification and clustering]{Keypoint verification and clustering. }
\label{ex1:thir}
\end{figure}

\begin{figure}[b]
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{VeriRgsimg00634.jpg}
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{VeriRgsimg01075.jpg}
\caption[Keypoint cluster verification by appearance]{Keypoint cluster verification by appearance. Red rectangles: positive detection hypotheses, and green: negative detection hypotheses.}
\label{ex1:four}
\end{figure}

\begin{figure}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{Trjimg00469.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{Trjimg01080.jpg}
}
\caption[Keypoint cluster tracking]{Keypoint cluster tracking.}
\label{ex1:five}
\end{figure}

\begin{figure}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{veriRegTrjimg00626.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{veriRegTrjimg00634.jpg}
}\\
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{veriRegTrjimg01072.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{veriRegTrjimg01080.jpg}
}
\caption[Detection results]{Detection results.}
\label{ex1:sixs}
\end{figure}

\subsubsection{Detection Results} On a laptop with Intel Core2 Duo 2.8GHz processors, the method deals with real data at a frame rate of 34 frames per second, and this fulfills real-time requirements.

The detection rate and false alarm rate is evaluated on 250 frames, as shown in Table \ref{ex1:tb2}.

\begin{table}[h]
\centering
\begin{tabular}{lll}
     \hline
     \hline
    Total number &	113   \\
    Correctly labeled &	102   \\
    Miss detections &	11 &	  \\
    False alarms &	21    \\
    Detection rate &	90\% &	  \\
    False alarm rate &	19\% &	   \\
   \hline
\end{tabular}
\caption{Detection rate and false alarm rate.}\label{ex1:tb2}
\end{table}

\subsection{Second Experiment}



\textbf{Data} The results of the first experiment is not satisfactory, and then the second experiment is carried out. A better far infrared camera is used, and also the zoom of the camera is adjusted for better images. Then with the new camera mounted on top, the experimental vehicle took several tours of the Awagatake tunnel. About 7,000 frames are collected for each tour. The frame size is $640\times 480$, the intensity range is [0,255], and the frame rate is 30 frames per second of the camera, also of the data collection program which is provided by the camera maker.

\textbf{Implementation Settings} The same with experiment one, all models are trained using data from one tour, and evaluated on data on another tour.

About intensity thresholds for keypoint detection, $I^{th1}_{\bf{x}}$ is set to 160 and $I^{th2}_{\bf{x}}$, 190. According to the sensed emergency telephone indicator, width step $W$ set to 3, and $H$ set to 4.

Instead of training a new mixture model for keypoint verification, the older one from experiment one is used, since the performance of this step is not critical.

For keypoint clustering, the threshold to split the mst is set to 40, which is half the largest height of the emergency telephone indicators sensed with new camera and experimental settings.

The Adaboost machine to distinguish other vehicles and their shadows is trained by intensity histograms of positive keypoint clusters and negative keypoint clusters.  If the Adaboost machine is trained by averagely weighted training examples as in experiment one, its correct rate on the training examples is overall 84\%. When trained using bias weighted training examples, which means the initial weight of the positive training examples are here $7$ times as large as those of the positive training examples, its correct rate on the positive training examples is 94\%, and 77\% on the negative training examples.

The main difference between experiment one, and experiment two in the pipeline lays here. The same weights are assigned to positive and negative training examples during training the Adaboost machine, while biased weights are assigned here. This will results in an Adaboost machine which gives better performance on target objects, and perform worse on noisy objects. The produced false alarms can be later filtered out by more powerful step which uses motion information.

During keypoint cluster tracking, whether a detection response can be linked to a trajectory or not is constrained by position and scale changes. Here scale change limit is set to 4. When the trajectories are fitted as lines, the linear model is also used in associating new detection responses.



\begin{figure}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{17veriTrjimg00039.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{8veriTrjimg00028.jpg}
}\\
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{19veriTrjimg00041.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{3veriTrjimg00023.jpg}
}\\
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{04veriTrjimg00039.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{02veriTrjimg00046.jpg}
}\\
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{00veriTrjimg00032.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 640 480]{06veriTrjimg00032.jpg}
}

\caption[Detection results]{Detection results.}
\label{fig:sixs}
\end{figure}

\textbf{Detection Results}

On an ordinary desktop computer with Intel Core2 Quad 2.6GHz processors, the method deals with real data at a frame rate of 41 frames per second, and this fulfills real-time requirements.


The detection rate and false alarm rate are evaluated on the keypoint clusters, as shown in Table \ref{tb:tb2}.
More detection results are shown in Figure \ref{fig:sixs}.
\begin{table}[h]
\centering
\begin{tabular}{lll}
     \hline
     \hline
    Total number &	472  \\
    Correctly labeled &	468   \\
    Miss detections &	4 &	  \\
    False alarms &	22    \\
    Detection rate &	99.2\% &	  \\
    False alarm rate &	4.4\% &	   \\
   \hline
\end{tabular}
\caption{Detection rate and false alarm rate.}\label{tb:tb2}
\end{table}

The detection rate and false alarm rate of the first experiment~\cite{wang1} are 90\% and 19\%, while evaluated on a much smaller dataset. Here the results are better than the first experiment, because this sensed images are much clearer, and also because this more effective training of the Adaboost machine. And in the second experiment, the efficiency results in better computer.

The results on the trajectories of decisions are also evaluated. We one trajectory disappears, if its length is larger than 15, and over 80\% of the decisions it connects are positive, it is considered as positive. The method correctly detects all the 22 emergency telephone indicators with no false alarms. The detection rate is 100\%, and the false alarm rate is 0\%, as shown in Table \ref{tb:tb3}.

\begin{table}[h]
\centering
\begin{tabular}{lll}
     \hline
     \hline
    total number & 22  \\
    correctly labeled &	22   \\
    miss detections &	0 &	  \\
    false alarms &	0    \\
    detection rate &	100\% &	  \\
    false alarm rate &	0\% &	   \\
   \hline
\end{tabular}
\caption[Final detection rate and false alarm rate]{Final detection rate and false alarm rate.}\label{tb:tb3}
\end{table}

\section{Experimental results on data collected by ordinary cameras}
\label{ord}




\begin{figure}
{
\includegraphics[width=0.47\textwidth,bb=0 0 720 576]{frame92_af.jpg}
}
{
\includegraphics[width=0.47\textwidth,bb=0 0 720 576]{frame92_af.jpg}
}\\

\caption[Keypoint verification]{Results of keypoint verification. Rectangles mark the manually marked ground-truth boxes. Pink points mark the points labeled as negative in keypoint verification step, others mark keypoints of the positive. }
\label{ord:two}
\end{figure}
The method is extended at the first step to deal with data collected by ordinary cameras. The task is to detect pedestrians and bicycle riders. SURF~\cite{surf} is used as keypoint detector and descriptor.

A new keypoint verification step is proposed. Keypoints from training examples are clustered using $k$-means as a mixture model. Gaussian distribution is assumed for each cluster, and the variances are also estimated, which will be used as criteria for keypoint verification. Different $k$s are used to generate the mixture model, and the performance is evaluated in Figure \ref{ord:one}.  An ensemble model is proposed, which never performs worst. All results of $k$-means with different parameters are summarised to produce the results of ensemble model.

As shown in Figure \ref{ord:two}, keypoint detection step is performing good in detect keypoints belonging to the target objects. The performance of keypoint verification is not good, as shown in Figure \ref{ord:one}. And this leads to infeasible later steps.

The failure of this method in complex scene is the lacking of descriptive power in its model. Beside appearance information, the positional information is also important when talking about complex scenes.

\begin{figure}
\centering
{
  \includegraphics[width=0.8\textwidth]{kptkms.eps}
}
\caption[Evaluation of keypoint verification]{Evaluation of keypoint verification. Keypoint verification performance of $k$-means mixture models with different parameters. Here, $k$ is the main parameter, while the model of ensemble is a mixture of all other $k$-means methods.}
\label{ord:one}
\end{figure}

\section{Chapter Conclusion}
\label{conc}
 This chapter proposes an object detection method, which performs well in simple scenarios by combining appearance and motion information in a very efficient way. The method makes use of appearance and motion information of the target objects in a hierarchical manner. With careful optimization of detection pipeline, the method gives promising results in real time.

  Also one main idea of the method is not to consider objects as something only with specific appearance patterns, but as something with both
 specific appearance and motion patterns. Another reason for the performance during the second experiment, is that make dangerous decisions later, when more information are available. While using both appearance and motion information results in a detection rate of about 99\%, combing all the information connected by a trajectory results in a detection rate of 100\%.

 Though its performance under complex scenarios is not promising, it can still be used as a unit in  positioning systems in tunnel environment.
