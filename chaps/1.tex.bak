\chapter{Introduction}
Detecting objects of interest from a complex scene is a basic perceptual skill in human beings and other animals.
While in the area of computer science, object detection~\cite{dod} is a computer technology that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. And successful object detection
methods play fundamental roles in a lot of application areas, which include video surveillance, driving assistance, image retrieval, etc.

Most modern detection methods fall into two categories. Some~\cite{ij4,ac31,ac30,ac4,ac32,ac29,ac28,ac1} follow the sliding-window schema, and they detect objects by consider whether each of the sub-images contains an instance of the target object. The other methods~\cite{ac9,ac2,ac3,ac22,lb1,ac5,ac10,ac21,ac18} infer object centers based on local image features in a bottom-up manner. These methods start with detection of object parts, in the form of image patches, edgelets, or keypoints, and then infer about the target objects' status, like position, or label. 


 Humans still far outperform computers in the tasks of image-based recognition and detection. Only a few techniques are mature enough for daily applications, i.e., face detection~\cite{face} used in cameras. For years, in computer vision, lots of researchers focus on object detection from images. Their efforts include proposing better image features~\cite{o17} or better models for object representation~\cite{bgf}, proposing better discriminative classifiers~\cite{dlearn} or better inference model~\cite{hdp}, or proposing better searching techniques for exploring solution space~\cite{bab}.
 
 In this thesis, efforts are also made to improve performance of detection methods. These efforts try to explore how to use the information which are not made full use of by previous methods. Roughly, the efforts belong to two categories, the first category is exploring approaches of efficiently and effectively combing of motion information with appearance information, and the second category is exploring mutual information encoded in image features of the same object.
 
 Most methods for detection mainly use either appearance information or motion information. And in our methods, we will address when the information from these two channels are well combined, detection performance can be much better. 
 
 The first method is developed mainly for real-time applications under limited computational power. This kind of scenarios include when using embedded sensors. This method can be considered as a two-step method. The first step deals with keypoints. It
takes original data as input, and outputs keypoint clusters as detection hypotheses. This step detects, verifies, and clusters keypoints. The second
step takes these keypoint clusters as input, verifies them by their appearance and motion
information, and outputs the ones which pass verifications as detection results. Motion information plays a very important role in the method. The target objects are considered as possessing both special appearance pattern and motion pattern. When the second step verifies the detection hypotheses using appearance information, a biased classifier is used. This classifier produce more false alarms to pursue higher detection rate. Then motion information is used to filter out the false alarms. Also the pipeline of this method is optimized in a hierarchical way, that the later one step is, the more time consuming it is, and the fewer instances it will deal with. The method performs well under simple scene, i.e., data collected by infrared cameras in tunnel environment, and gives 100\% detection rate 0\% false alarm rate in one of the experiments. However, the performance of this method under complicated scene is not promising. And then we propose the second method.

The second method belong to methods based on Hough transforms. It extend the Implicit Shape Model~\cite{lb1} to combine motion information. For training, image features together with labels and offsets to object centers of sample images are considered as codes, and inserted into a codebook. For detection, image features are detected on the target image, and then matched against the codebook using image feature as key. The matched codes will indicate the labels and object centers. During the detection step, this method first do motion analysis, which results in grouping results of the image features on the target image. The grouping results are used during the inference for labels and centers of the target objects. An assumption that image features with the same motion pattern, here in the same motion group, should belong to the same object. The inference procedure then prefer the label and position infers with more consistence in the motion group. On two datasets, the proposed method outperform the state-of-the-art method.

While the second method performs well under complicated scene, it cannot give results in real time. This is due to the time-consuming property of methods based on Hough transforms.   The third method aims at improving the efficiency of the second method. Also it tries to flatten the gap of appearance and positional information. This method does not use motion information. In methods based on Hough transforms, image features are used as key to query similar codes from the codebook, and in the third method, both appearance and position are used as key. The bottom-up property of Hough transform also ignore the relationship between different image features. Actually, the mutual information encoded in the image features of the same object is very informational. The third method consider objects as point sets of, i.e., 14-dimensional, while the first 12 dimensions are appearance information, and the last 2 dimensions are positional information. The training step is almost the same with Hough-transform methods, except for how a few parameters are trained. At the detection step, instead of using the appearance information of one single feature as query, the point set of a sub-image is used as query. Pyramid Matching is used for accelerating the querying. The procedure ensures the use of the mutual information encoded in the image features of the same object.

The thesis is organised as follows. In chapter 2, application background and some very related work are reviewed. In chapter 3, the method aimed at efficient detection by combining motion and appearance information is introduced. In chapter 4, the method which extends the Implicit Shape Model to incorporate motion information is proposed, this method groups object parts for detection. In chapter 5, the method which detects by Pyramid Matching Score is presented. Chapter 6 concludes, and discusses about possible improvements of the proposed methods for future work.



